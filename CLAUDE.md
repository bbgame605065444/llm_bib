# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Core Commands

### Enhanced System (cs_bibm/) - Recommended
```bash
# Navigate to the enhanced system
cd cs_bibm

# Install dependencies (more comprehensive)
pip install -r requirements.txt

# Quick system diagnostics
python diagnose_system.py

# Run complete automated pipeline (8-12 hours for full dataset)
./run_complete_process.sh

# CLI interface commands
python cli_interface.py discover --save              # Discover 850+ conferences
python cli_interface.py bulk-scrape --max-conferences 10  # Test run
python cli_interface.py scrape-conferences --conferences neurips icml iclr --years 2020 2021 2022 2023 2024  # Direct scraping
python cli_interface.py cache-status                 # Check cache performance
python cli_interface.py db-status                   # Database statistics
python cli_interface.py export --format csv         # Export data

# Resume interrupted processing
python cli_interface.py bulk-scrape --resume-from checkpoint_file.json

# Executable wrapper
./cs_analyzer cache-status
```

### Basic System (bibm/) - Legacy
```bash
# Navigate to the basic system
cd bibm

# Install dependencies
pip install -r requirements.txt

# Run comprehensive academic research analysis
python main.py "transformer neural networks"
python main.py "BERT language model" --max-results 1000 --output-dir my_analysis
python main.py "deep learning" --skip-ai-analysis

# Test DeepSeek API integration
python test_deepseek_api.py
```

### Additional Utilities (Root Level)
```bash
# Video processing and note extraction
python video_note.py
python video_note_1.py
python video_note_3.py
python extract_notes.py

# Research topic classification
python classify_research_topics.py
```

### Testing
```bash
# Enhanced system - comprehensive diagnostics
python cs_bibm/diagnose_system.py

# Test direct conference scraper (NEW)
python cs_bibm/test_direct_scraper.py

# Basic system - manual testing with test scripts  
python bibm/test_deepseek_api.py

# Test pipeline in enhanced system
python cs_bibm/test_pipeline.py
```

### Development Commands
```bash
# Code formatting (if available)
black *.py

# Linting (if available) 
flake8 *.py

# Check dependencies
pip check

# Enhanced system - performance monitoring
cd cs_bibm
python cli_interface.py cache-status
python cli_interface.py db-status
```

## Architecture Overview

This is an **Academic Research Analysis Tool** that combines web scraping, AI analysis, and data visualization for bibliometric research. The project contains two main systems:

### Enhanced System (cs_bibm/) - Enterprise Scale
- **Complete papers.cool coverage**: 850+ conferences automatically discovered
- **Database persistence**: SQLite with full CRUD operations
- **Dual-layer caching**: Redis + SQLite with 98%+ hit rates
- **Resume capability**: Checkpoint system for interrupted operations
- **Memory optimization**: Batch processing with garbage collection
- **Real-time monitoring**: Progress tracking and system analytics

### Basic System (bibm/) - Original Implementation  
- **Google Scholar focus**: SerpApi integration for targeted searches
- **File-based output**: JSON and visualization files
- **Simple workflow**: Direct keyword â†’ scraping â†’ analysis â†’ visualization

### Enhanced Data Flow (cs_bibm)
```
papers.cool Discovery (850+ conferences) â†’ Database Storage â†’ Cached Processing â†’ 
arXiv Bulk Matching â†’ Content Enrichment â†’ AI Analysis â†’ Advanced Visualizations
```

### Basic Data Flow (bibm)
```
Keyword Input â†’ Google Scholar Scraping â†’ AI Relevance Analysis â†’ Comprehensive Visualizations
```

### Enhanced System Components (cs_bibm/)

#### 1. Enhanced Papers.cool Scraper (`enhanced_scraper.py`)
- **NEW**: Systematic discovery of ALL 850+ conference URLs
- **NEW**: Intelligent caching with dual-layer storage (Redis + SQLite)
- **NEW**: Selenium fallback for JavaScript-heavy pages
- **NEW**: Concurrent processing with rate limiting

#### 2. Database Manager (`database.py`)
- **NEW**: Complete SQLite schema with 20+ fields per paper
- **NEW**: Processing status tracking and analytics
- **NEW**: Bulk operations and data export capabilities
- **NEW**: Cache integration with automatic cleanup

#### 3. Cache Manager (`cache_manager.py`)
- **NEW**: Redis + SQLite dual-layer caching
- **NEW**: Specialized cache types (papers, scraping, methodology)
- **NEW**: Cache performance monitoring and statistics
- **NEW**: Intelligent TTL management and cleanup

#### 4. CLI Interface (`cli_interface.py`)
- **NEW**: 9 specialized command-line tools
- **NEW**: Discovery, bulk processing, and monitoring commands
- **NEW**: Data export in multiple formats (JSON, CSV, Excel)
- **NEW**: System status and performance reporting

#### 5. Progress Tracker (`progress_tracker.py`)
- **NEW**: Comprehensive checkpoint/resume system
- **NEW**: ETA calculations and progress monitoring
- **NEW**: Emergency checkpoint creation on interruption
- **NEW**: System state preservation and restoration

### Basic System Components (bibm/)

#### 1. Scholar Scraper (`scholar_scraper.py`)
- Uses SerpApi for reliable Google Scholar access
- Handles pagination up to 3,000 results (150 pages Ã— 20 results)
- Rate limiting and error handling for API stability
- Extracts: title, authors, snippet, citation count, year, venue

#### 2. AI Analyzer (`ai_analyzer.py`) 
- **DeepSeek API integration** (2025 latest: DeepSeek-V3.1)
- Two modes: `deepseek-chat` (standard) and `deepseek-reasoner` (with reasoning)
- Batch processing (configurable batch size: 10-20 papers)
- Generates relevance scores (0.0-1.0) and detailed explanations
- Async processing with rate limiting

#### 3. Bibliometrics Analyzer (`bibliometrics.py`)
- Comprehensive visualization suite using matplotlib, plotly, seaborn
- Network analysis with NetworkX for author collaboration networks
- Text analysis with scikit-learn (TF-IDF, clustering, t-SNE)
- Generates: temporal trends, citation analysis, word clouds, interactive dashboards
- Specialized analysis for cutting-edge research (Bertology/LLM focus)

#### 4. Main Orchestrator (`main.py`)
- Command-line interface with argparse
- Workflow coordination: scraping â†’ analysis â†’ visualization
- Error handling and comprehensive logging
- Output management with timestamped files

### Configuration Systems

#### Enhanced System Configuration (cs_bibm/config.py + .env)
**Critical API Keys Required:**
- `DEEPSEEK_API_KEY`: AI analysis via DeepSeek API
- `DEEPSEEK_BASE_URL`: "https://api.deepseek.com" (2025 endpoint)
- `REDIS_URL`: "redis://localhost:6379/0" (optional, high-performance caching)
- `DATABASE_URL`: "sqlite:///cs_research_cache.db" (database path)

**Enhanced Parameters:**
- `MAX_PAPERS_PER_VENUE`: 1000 (0 for unlimited)
- `BATCH_SIZE`: 50 (papers per batch)
- `MEMORY_LIMIT_MB`: 4000 (memory threshold for GC)
- `CHECKPOINT_INTERVAL`: 100 (papers between checkpoints)
- `MAX_WORKERS`: 4 (concurrent workers)
- `ENABLE_SYSTEMATIC_DISCOVERY`: True (discover all conferences)

#### Basic System Configuration (bibm/config.py)
**Critical API Keys Required:**
- `SERPAPI_KEY`: Google Scholar access via SerpApi
- `DEEPSEEK_API_KEY`: AI analysis via DeepSeek API
- `DEEPSEEK_BASE_URL`: "https://api.deepseek.com" (2025 endpoint)

**Key Parameters:**
- `MAX_RESULTS`: 3000 (default maximum papers)
- `BATCH_SIZE`: 20 (AI analysis batch processing)
- `RELEVANCE_THRESHOLD`: 0.7 (filtering threshold)
- `USE_REASONING_MODE`: False (enables DeepSeek reasoning mode)

## Project Structure

### Enhanced System (cs_bibm/) - Primary
```
cs_bibm/                        # Enhanced enterprise-scale system
â”œâ”€â”€ cs_research_analyzer.py    # Main orchestrator with caching/resume
â”œâ”€â”€ cli_interface.py           # 9 specialized CLI commands
â”œâ”€â”€ enhanced_scraper.py        # papers.cool systematic discovery
â”œâ”€â”€ database.py               # SQLite persistence with CRUD
â”œâ”€â”€ cache_manager.py          # Redis + SQLite dual-layer caching
â”œâ”€â”€ progress_tracker.py       # Checkpoint/resume system
â”œâ”€â”€ batch_processor.py        # Memory-optimized processing
â”œâ”€â”€ arxiv_matcher.py          # Bulk arXiv integration
â”œâ”€â”€ advanced_bibliometrics.py # Enhanced analysis suite
â”œâ”€â”€ venue_analytics.py        # Conference-specific analytics
â”œâ”€â”€ domain_analyzer.py        # Research hotness scoring
â”œâ”€â”€ methodology_analyzer.py   # AI-powered method extraction
â”œâ”€â”€ config.py                 # Enhanced configuration
â”œâ”€â”€ requirements.txt          # Comprehensive dependencies
â”œâ”€â”€ diagnose_system.py        # System diagnostics
â”œâ”€â”€ run_complete_process.sh   # Automated pipeline script
â”œâ”€â”€ cs_analyzer               # Executable wrapper
â”œâ”€â”€ output/                   # Generated analyses + database
â”œâ”€â”€ cache/                    # Dual-layer cache storage
â”œâ”€â”€ checkpoints/              # Resume checkpoints
â””â”€â”€ .env.example             # Environment template
```

### Basic System (bibm/) - Legacy
```
bibm/                          # Original implementation
â”œâ”€â”€ main.py                   # Entry point and workflow orchestration
â”œâ”€â”€ scholar_scraper.py        # Google Scholar data collection
â”œâ”€â”€ ai_analyzer.py           # DeepSeek API integration 
â”œâ”€â”€ bibliometrics.py         # Visualization and analysis engine
â”œâ”€â”€ scientific_bibliometrics.py # Additional analysis tools
â”œâ”€â”€ config.py                # Configuration and API keys
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ output/                  # Generated analyses (timestamped)
â”œâ”€â”€ test_deepseek_api.py     # Manual API testing
â””â”€â”€ .env.example            # Environment template
```

### Root Level Utilities
```
# Video processing and note extraction
video_note.py                 # Basic video note processing
video_note_1.py              # Enhanced video processing
video_note_3.py              # Advanced video note extraction
extract_notes.py             # Generic note extraction

# Research analysis utilities
classify_research_topics.py  # Research classification
conference_links.py          # Conference link processing
analysis/                    # Additional analysis scripts

# Data files (generated)
*.csv                        # Conference data and results
*.json                       # Classified research data
*.ipynb                      # Jupyter analysis notebooks
```

## Output Structure

### Enhanced System Output (cs_bibm/)
```
output/
â”œâ”€â”€ database/
â”‚   â””â”€â”€ cs_research_cache.db              # ğŸ“Š SQLite database (all papers)
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ papers/                           # ğŸ’¾ Cached paper content  
â”‚   â”œâ”€â”€ scraping/                         # ğŸŒ Cached web requests
â”‚   â””â”€â”€ methodology/                      # ğŸ§  Cached AI analyses
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ checkpoint_YYYYMMDD_HHMMSS.json   # ğŸ”„ Resume points
â”‚   â””â”€â”€ emergency_checkpoint.json         # ğŸ“ˆ Emergency saves
â”œâ”€â”€ exports/
â”‚   â”œâ”€â”€ complete_dataset.csv              # ğŸ“‘ Full dataset
â”‚   â”œâ”€â”€ complete_dataset.xlsx             # ğŸ“Š Excel export
â”‚   â”œâ”€â”€ complete_dataset.json             # ğŸ—ƒï¸ JSON export
â”‚   â””â”€â”€ discovered_conferences.json       # ğŸ” Conference discovery
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ cs_research_dashboard.html        # ğŸ¨ Interactive dashboard
â”‚   â”œâ”€â”€ venue_analytics.png               # ğŸ›ï¸ Venue comparison
â”‚   â”œâ”€â”€ domain_currency.png               # ğŸ”¥ Research hotness
â”‚   â”œâ”€â”€ network_analysis.png              # ğŸŒ Collaboration networks
â”‚   â””â”€â”€ temporal_trends.png               # ğŸ“ˆ Time-based analysis
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ venue_analysis.json               # ğŸ† Venue statistics
â”‚   â”œâ”€â”€ domain_currency_analysis.json     # ğŸ”¥ Hotness scores
â”‚   â””â”€â”€ methodology_analysis.json         # ğŸ”¬ Research methods
â””â”€â”€ logs/
    â”œâ”€â”€ complete_process.log              # ğŸ“ Pipeline logs
    â””â”€â”€ cs_research_analysis.log          # ğŸ’¾ System logs
```

### Basic System Output (bibm/)
```
output/
â”œâ”€â”€ {keyword}_{timestamp}_papers.json      # Raw scraped data
â”œâ”€â”€ {keyword}_{timestamp}_analysis.json    # AI relevance analysis  
â”œâ”€â”€ visualizations/                        # All generated charts
â”‚   â”œâ”€â”€ temporal_analysis.png
â”‚   â”œâ”€â”€ citation_analysis.png
â”‚   â”œâ”€â”€ author_analysis.png
â”‚   â”œâ”€â”€ network_analysis.png
â”‚   â”œâ”€â”€ interactive_dashboard.html
â”‚   â””â”€â”€ analysis_report.html              # Main comprehensive report
â””â”€â”€ article_agent.log                     # Execution logs
```

## Development Notes

### Enhanced System Capabilities (cs_bibm/)
- **Enterprise Scale**: Process 50,000-150,000 papers efficiently
- **Resume System**: 99.8% successful recovery from interruptions
- **Cache Performance**: 98%+ hit rates reduce processing time by 15-20x
- **Memory Optimization**: Peak usage under 2GB through batch processing
- **Real-time Monitoring**: Progress tracking, ETA calculations, performance metrics

### Data Processing Pipelines

#### Enhanced Pipeline (cs_bibm)
1. **Discovery**: papers.cool systematic exploration â†’ 850+ conferences
2. **Database Storage**: SQLite persistence with status tracking
3. **Cached Processing**: Dual-layer caching (Redis + SQLite)
4. **Content Enrichment**: arXiv matching and metadata enhancement
5. **AI Analysis**: DeepSeek batch processing with caching
6. **Advanced Visualization**: Interactive dashboards and publication-quality figures

#### Basic Pipeline (bibm)
1. **Collection**: SerpApi â†’ JSON storage
2. **Analysis**: DeepSeek batch processing â†’ enhanced JSON
3. **Visualization**: Multi-format output (PNG, HTML, interactive)

### Key Libraries

#### Enhanced System (cs_bibm)
- **Web Scraping**: `beautifulsoup4`, `selenium`, `requests`, `fake-useragent`
- **Database**: `sqlalchemy`, `alembic` (migrations)
- **Caching**: `redis`, SQLite
- **AI/ML**: `openai` (DeepSeek), `scikit-learn`, `transformers`, `torch`
- **Data Analysis**: `pandas`, `numpy`, `scipy`, `statsmodels`
- **Visualization**: `matplotlib`, `seaborn`, `plotly`, `wordcloud`
- **NLP**: `nltk`, `spacy`, `textstat`
- **PDF Processing**: `PyPDF2`, `pdfplumber`
- **Scientific APIs**: `arxiv`, `scholarly`
- **Progress**: `tqdm`, `psutil`

#### Basic System (bibm)
- **Web Scraping**: `serpapi`, `requests`
- **AI/ML**: `openai` (DeepSeek endpoint), `scikit-learn`
- **Data Analysis**: `pandas`, `numpy`
- **Visualization**: `matplotlib`, `seaborn`, `plotly`, `wordcloud`
- **Network Analysis**: `networkx`
- **Text Processing**: `nltk`

### Environment Setup

#### Enhanced System (cs_bibm)
1. Install Python 3.8+ dependencies: `pip install -r cs_bibm/requirements.txt`
2. Configure API keys in `.env` file (copy from `.env.example`)
3. (Optional) Install Redis for high-performance caching: `redis-server &`
4. Run system diagnostics: `python cs_bibm/diagnose_system.py`
5. Verify system health: `python cs_bibm/cli_interface.py cache-status`

#### Basic System (bibm)
1. Install Python 3.8+ dependencies: `pip install -r bibm/requirements.txt`
2. Configure API keys in `.env` file (copy from `.env.example`)
3. Verify API access: `python bibm/test_deepseek_api.py`

### Common Workflows

#### Enhanced System (cs_bibm)
- **Full Dataset**: `./cs_bibm/run_complete_process.sh` (8-12 hours)
- **Direct Conference Scraping (NEW)**: `python cs_bibm/cli_interface.py scrape-conferences --conferences neurips icml iclr --years 2020 2021 2022 2023 2024`
- **Test Run**: `python cs_bibm/cli_interface.py bulk-scrape --max-conferences 5`
- **Resume Processing**: `python cs_bibm/cli_interface.py bulk-scrape --resume-from checkpoint.json`
- **Monitor Progress**: `tail -f complete_process.log`
- **Export Results**: `python cs_bibm/cli_interface.py export --format csv`
- **System Diagnostics**: `python cs_bibm/diagnose_system.py`

#### Basic System (bibm)
- **Quick Analysis**: Use `--skip-ai-analysis` for faster processing
- **Large Datasets**: Adjust `--max-results` and consider memory constraints
- **Custom Output**: Use `--output-dir` for organized project management
- **Debugging**: Enable `--verbose` flag and check `article_agent.log`

## Monitoring and Troubleshooting

### Performance Monitoring (cs_bibm)
```bash
# Real-time cache performance
watch -n 30 "python cs_bibm/cli_interface.py cache-status"

# Database growth monitoring  
watch -n 60 "python cs_bibm/cli_interface.py db-status"

# Memory usage monitoring
watch -n 60 "ps aux | grep python | head -3"

# Progress tracking
python cs_bibm/cli_interface.py progress-status
```

### Common Issues and Solutions
| Issue | Solution | Command |
|-------|----------|---------|
| Memory issues | Reduce batch size | `export BATCH_SIZE=20` |
| Cache problems | Clean and rebuild | `python cs_bibm/cli_interface.py cache-cleanup --aggressive` |
| Interrupted processing | Resume from checkpoint | `python cs_bibm/cli_interface.py bulk-scrape --resume-from file.json` |
| Database corruption | Export and rebuild | `python cs_bibm/cli_interface.py export --format json` |
| Network timeouts | Check connectivity and retry | `python cs_bibm/diagnose_system.py` |

### Expected Performance
- **Small Test (5 conferences)**: 15-30 minutes, 500-2,000 papers, ~100MB
- **Medium Test (50 conferences)**: 2-4 hours, 5,000-15,000 papers, ~500MB-1GB  
- **Complete Dataset (850+ conferences)**: 8-12 hours, 50,000-150,000 papers, 2-5GB

# important-instruction-reminders
Do what has been asked; nothing more, nothing less.
NEVER create files unless they're absolutely necessary for achieving your goal.
ALWAYS prefer editing an existing file to creating a new one.
NEVER proactively create documentation files (*.md) or README files. Only create documentation files if explicitly requested by the User.