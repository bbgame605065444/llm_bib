import gradio as gr
import cv2
import numpy as np
# from moviepy.editor import VideoFileClip
from moviepy import *
import whisper
import torch
import json
import os
from datetime import timedelta
import tempfile
from PIL import Image
from transformers import CLIPProcessor, CLIPModel, AutoModelForCausalLM, AutoTokenizer
import re
from typing import List, Tuple, Dict
import gc
from pydub import AudioSegment
import logging
import ollama  # æ·»åŠ è¿™ä¸€è¡Œ

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VideoProcessor:
    def __init__(self):
        self.video_path = None
        self.roi = None
        self.whisper_model = None
        self.clip_model = None
        self.clip_processor = None
        self.gemma_model = None
        self.gemma_tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {self.device}")
        
    def extract_audio(self, video_path):
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
        try:
            video = VideoFileClip(video_path)
            audio = video.audio
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            audio_path = tempfile.mktemp(suffix=".wav")
            audio.write_audiofile(audio_path)
            
            video.close()
            return audio_path
        except Exception as e:
            logger.error(f"Error extracting audio: {e}")
            raise
    
    def split_audio_chunks(self, audio_path, chunk_duration=3600, overlap=60):
        """å°†éŸ³é¢‘åˆ†å‰²æˆ1å°æ—¶çš„å—ï¼Œå¸¦é‡å """
        try:
            audio = AudioSegment.from_wav(audio_path)
            chunks = []
            
            chunk_length = chunk_duration * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            overlap_length = overlap * 1000
            
            start = 0
            chunk_num = 0
            
            while start < len(audio):
                end = min(start + chunk_length, len(audio))
                chunk = audio[start:end]
                
                chunk_path = tempfile.mktemp(suffix=f"_chunk_{chunk_num}.wav")
                chunk.export(chunk_path, format="wav")
                chunks.append((chunk_path, start/1000))  # è¿”å›è·¯å¾„å’Œå¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
                
                start += chunk_length - overlap_length
                chunk_num += 1
                
            return chunks
        except Exception as e:
            logger.error(f"Error splitting audio: {e}")
            raise
    
    def transcribe_audio(self, audio_chunks):
        """ä½¿ç”¨Whisperè½¬å½•éŸ³é¢‘"""
        try:
            if self.whisper_model is None:
                logger.info("Loading Whisper model...")
                self.whisper_model = whisper.load_model("base")
            
            all_segments = []
            
            for chunk_path, start_time in audio_chunks:
                logger.info(f"Transcribing chunk starting at {start_time}s...")
                result = self.whisper_model.transcribe(chunk_path)
                
                # è°ƒæ•´æ—¶é—´æˆ³
                for segment in result["segments"]:
                    segment["start"] += start_time
                    segment["end"] += start_time
                    all_segments.append(segment)
            
            # ä¿å­˜åˆ°JSON
            json_path = tempfile.mktemp(suffix=".json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(all_segments, f, ensure_ascii=False, indent=2)
            
            # å¸è½½æ¨¡å‹ä»¥é‡Šæ”¾å†…å­˜
            logger.info("Offloading Whisper model...")
            del self.whisper_model
            self.whisper_model = None
            gc.collect()
            if self.device == "cuda":
                torch.cuda.empty_cache()
            
            return json_path, all_segments
        except Exception as e:
            logger.error(f"Error transcribing audio: {e}")
            raise
    
    def get_frame_at_time(self, video_path, time_seconds):
        """è·å–æŒ‡å®šæ—¶é—´çš„è§†é¢‘å¸§"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_number = int(fps * time_seconds)
        
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
        ret, frame = cap.read()
        cap.release()
        
        if ret:
            return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        return None
    
    def extract_frames_with_roi(self, video_path, interval_seconds, roi):
        """æ ¹æ®ROIæå–è§†é¢‘å¸§"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        frames = []
        frame_numbers = []
        
        logger.info(f"Extracting frames every {interval_seconds}s from {duration:.1f}s video...")
        
        for i in range(0, int(duration), interval_seconds):
            frame_num = int(i * fps)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
            ret, frame = cap.read()
            
            if ret:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                if roi:
                    x1, y1, x2, y2 = roi
                    frame = frame[y1:y2, x1:x2]
                frames.append(frame)
                frame_numbers.append(frame_num)
        
        cap.release()
        logger.info(f"Extracted {len(frames)} frames")
        return frames, frame_numbers
    
    def remove_duplicate_frames(self, frames, frame_numbers, threshold=0.95):
        """ä½¿ç”¨CLIPç§»é™¤é‡å¤å¸§"""
        try:
            if self.clip_model is None:
                logger.info("Loading CLIP model from Hugging Face...")
                self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
            
            unique_frames = []
            unique_numbers = []
            
            if len(frames) == 0:
                return unique_frames, unique_numbers
            
            # è½¬æ¢æ‰€æœ‰å¸§ä¸ºPILå›¾åƒ
            pil_images = [Image.fromarray(frame) for frame in frames]
            
            # æ‰¹é‡å¤„ç†å›¾åƒè·å–ç‰¹å¾
            logger.info("Computing CLIP features for all frames...")
            all_features = []
            batch_size = 8  # æ ¹æ®GPUå†…å­˜è°ƒæ•´
            
            for i in range(0, len(pil_images), batch_size):
                batch = pil_images[i:i+batch_size]
                inputs = self.clip_processor(images=batch, return_tensors="pt").to(self.device)
                
                with torch.no_grad():
                    image_features = self.clip_model.get_image_features(**inputs)
                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                    all_features.append(image_features)
            
            all_features = torch.cat(all_features, dim=0)
            
            # æ¯”è¾ƒç›¸ä¼¼åº¦å¹¶å»é‡
            unique_indices = [0]  # ç¬¬ä¸€å¸§æ€»æ˜¯ä¿ç•™
            
            for i in range(1, len(frames)):
                is_duplicate = False
                current_feature = all_features[i:i+1]
                
                for j in unique_indices:
                    similarity = torch.cosine_similarity(current_feature, all_features[j:j+1])
                    if similarity.item() > threshold:
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    unique_indices.append(i)
            
            # æ”¶é›†å”¯ä¸€å¸§
            unique_frames = [frames[i] for i in unique_indices]
            unique_numbers = [frame_numbers[i] for i in unique_indices]
            
            logger.info(f"Removed {len(frames) - len(unique_frames)} duplicate frames")
            return unique_frames, unique_numbers
            
        except Exception as e:
            logger.error(f"Error removing duplicates: {e}")
            raise
    
    def extract_ppt_content(self, frame, frame_number, fps):
        """ä½¿ç”¨Ollamaå’ŒGemmaæå–PPTå†…å®¹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # ä¿å­˜å¸§åˆ°ä¸´æ—¶æ–‡ä»¶
            temp_frame_path = tempfile.mktemp(suffix=".png")
            pil_frame = Image.fromarray(frame)
            pil_frame.save(temp_frame_path)
            
            # æ„å»ºæç¤ºè¯ï¼Œè¦æ±‚æ¨¡å‹åˆ†æå›¾åƒä¸­çš„PPTå†…å®¹
            time_seconds = frame_number / fps
            time_str = str(timedelta(seconds=int(time_seconds)))
            
            prompt = f"""OCR the provided image and extract any presentation (PPT) content you can identify. \n\nPlease structure your response in markdown format"""
            # :\n1. A timestamp header indicating when in the video this frame appears (Time: {time_str})\n2. A 'Main Content' section describing the key points\n3. Any 'Text Detected' in the image\n4. Any 'Formulas' written in LaTeX format\n5. A 'Key Points' section as a bullet list of important concepts\n\nIf you cannot identify any presentation content, please respond with \"No presentation content identified at time {time_str}.\"
            # ä½¿ç”¨Ollamaè°ƒç”¨Gemmaæ¨¡å‹åˆ†æå›¾åƒ
            # ä½¿ç”¨gemma2:27bæ¨¡å‹ï¼Œå®ƒæ˜¯Gemmaçš„27Bç‰ˆæœ¬ï¼ŒåŠŸèƒ½æ›´å¼ºå¤§
            response = ollama.generate(
                model='gemma3n:e4b',
                prompt=prompt,
                images=[temp_frame_path],
                options={
                    'temperature': 0.1,  # ä½æ¸©åº¦å€¼ç¡®ä¿æ›´ä¸€è‡´çš„è¾“å‡º
                    'num_ctx': 2048      # å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦ä»¥å¤„ç†è¯¦ç»†å†…å®¹
                }
            )
            
            # æå–ç”Ÿæˆçš„å†…å®¹
            content = response['response'].strip()
            
            # å¦‚æœæ²¡æœ‰è¿”å›å†…å®¹ï¼Œæä¾›é»˜è®¤æ ¼å¼
            if not content:
                content = f"### Time: {time_str}\n\nNo presentation content identified.\n\n"
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_frame_path)
            
            return content
            
        except Exception as e:
            logger.error(f"Error extracting PPT content: {e}")
            time_seconds = frame_number / fps
            time_str = str(timedelta(seconds=int(time_seconds)))
            return f"### Time: {time_str}\n\nError extracting content: {str(e)}\n\n"

# Gradioç•Œé¢
class GradioInterface:
    def __init__(self):
        self.processor = VideoProcessor()
        self.preview_frame = None
        self.clicks = []
        self.video_path = None
        
    def upload_video(self, video):
        """å¤„ç†è§†é¢‘ä¸Šä¼ """
        if video is None:
            return None, None, None
        
        self.video_path = video
        
        # è·å–ç¬¬2åˆ†é’Ÿçš„é¢„è§ˆå¸§
        frame = self.processor.get_frame_at_time(video, 120)  # 2åˆ†é’Ÿ = 120ç§’
        
        if frame is not None:
            self.preview_frame = frame
            self.clicks = []  # é‡ç½®ç‚¹å‡»
            return frame, "è§†é¢‘ä¸Šä¼ æˆåŠŸï¼è¯·åœ¨é¢„è§ˆå¸§ä¸Šç‚¹å‡»ä¸¤æ¬¡æ¥é€‰æ‹©æ„Ÿå…´è¶£åŒºåŸŸã€‚", None
        else:
            # å¦‚æœè§†é¢‘ä¸è¶³2åˆ†é’Ÿï¼Œè·å–ä¸­é—´å¸§
            cap = cv2.VideoCapture(video)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps
            cap.release()
            
            middle_time = duration / 2
            frame = self.processor.get_frame_at_time(video, middle_time)
            if frame is not None:
                self.preview_frame = frame
                self.clicks = []
                return frame, f"è§†é¢‘é•¿åº¦ä¸è¶³2åˆ†é’Ÿï¼Œæ˜¾ç¤ºç¬¬{middle_time:.1f}ç§’çš„å¸§ã€‚è¯·ç‚¹å‡»ä¸¤æ¬¡é€‰æ‹©åŒºåŸŸã€‚", None
            
            return None, "æ— æ³•è¯»å–è§†é¢‘å¸§", None
    
    def handle_click(self, evt: gr.SelectData):
        """å¤„ç†é¼ æ ‡ç‚¹å‡»äº‹ä»¶"""
        if self.preview_frame is None:
            return self.preview_frame, "è¯·å…ˆä¸Šä¼ è§†é¢‘", None
        
        x, y = evt.index[0], evt.index[1]
        self.clicks.append((x, y))
        
        # åœ¨å›¾åƒä¸Šç»˜åˆ¶ç‚¹å‡»ä½ç½®
        img = self.preview_frame.copy()
        for click in self.clicks:
            cv2.circle(img, click, 5, (255, 0, 0), -1)
        
        if len(self.clicks) == 2:
            # ç»˜åˆ¶çŸ©å½¢
            x1, y1 = self.clicks[0]
            x2, y2 = self.clicks[1]
            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # ä¿å­˜ROI
            self.processor.roi = (
                min(x1, x2), min(y1, y2),
                max(x1, x2), max(y1, y2)
            )
            
            status = f"å·²é€‰æ‹©åŒºåŸŸ: ({min(x1,x2)}, {min(y1,y2)}) åˆ° ({max(x1,x2)}, {max(y1,y2)})"
            self.clicks = []  # é‡ç½®ç‚¹å‡»
        else:
            status = f"å·²ç‚¹å‡»ç¬¬{len(self.clicks)}ä¸ªç‚¹ï¼Œè¯·å†ç‚¹å‡»ä¸€æ¬¡å®Œæˆé€‰æ‹©"
        
        return img, status, None
    
    def extract_audio_only(self, video):
        """ä»…æå–éŸ³é¢‘"""
        if video is None:
            return None, "è¯·å…ˆä¸Šä¼ è§†é¢‘"
        
        try:
            audio_path = self.processor.extract_audio(video)
            return audio_path, "éŸ³é¢‘æå–æˆåŠŸï¼"
        except Exception as e:
            return None, f"éŸ³é¢‘æå–å¤±è´¥: {str(e)}"
    
    def process_video_full(self, video, interval, threshold, progress=gr.Progress()):
        """å®Œæ•´å¤„ç†è§†é¢‘"""
        if video is None:
            return None, None, None, "è¯·å…ˆä¸Šä¼ è§†é¢‘"
        
        try:
            progress(0.1, desc="æå–éŸ³é¢‘...")
            audio_path = self.processor.extract_audio(video)
            
            progress(0.2, desc="åˆ†å‰²éŸ³é¢‘...")
            audio_chunks = self.processor.split_audio_chunks(audio_path)
            
            progress(0.3, desc="è½¬å½•éŸ³é¢‘ï¼ˆè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...")
            transcript_json, segments = self.processor.transcribe_audio(audio_chunks)
            
            progress(0.5, desc="æå–è§†é¢‘å¸§...")
            frames, frame_numbers = self.processor.extract_frames_with_roi(
                video, interval, self.processor.roi
            )
            
            progress(0.7, desc="å»é™¤é‡å¤å¸§...")
            unique_frames, unique_numbers = self.processor.remove_duplicate_frames(
                frames, frame_numbers, threshold
            )
            
            progress(0.8, desc="æå–PPTå†…å®¹...")
            
            # è·å–è§†é¢‘ä¿¡æ¯
            cap = cv2.VideoCapture(video)
            fps = cap.get(cv2.CAP_PROP_FPS)
            cap.release()
            
            # åˆ›å»ºmarkdownå†…å®¹
            markdown_content = "# è§†é¢‘PPTå†…å®¹æå–æŠ¥å‘Š\n\n"
            markdown_content += f"- æ€»å¸§æ•°: {len(frames)}\n"
            markdown_content += f"- å”¯ä¸€å¸§æ•°: {len(unique_frames)}\n"
            markdown_content += f"- éŸ³é¢‘æ®µæ•°: {len(audio_chunks)}\n"
            markdown_content += f"- è½¬å½•æ®µæ•°: {len(segments)}\n\n"
            markdown_content += "---\n\n"
            
            # æ·»åŠ è½¬å½•å†…å®¹æ‘˜è¦
            markdown_content += "## éŸ³é¢‘è½¬å½•æ‘˜è¦\n\n"
            for i, segment in enumerate(segments[:5]):  # åªæ˜¾ç¤ºå‰5æ®µ
                markdown_content += f"- [{segment['start']:.1f}s - {segment['end']:.1f}s]: {segment['text']}\n"
            if len(segments) > 5:
                markdown_content += f"\n*...è¿˜æœ‰ {len(segments)-5} æ®µè½¬å½•å†…å®¹ï¼Œè¯¦è§JSONæ–‡ä»¶*\n"
            
            markdown_content += "\n---\n\n"
            markdown_content += "## PPTå†…å®¹æå–\n\n"
            
            # å¤„ç†æ¯ä¸€å¸§
            for i, (frame, frame_num) in enumerate(zip(unique_frames, unique_numbers)):
                progress(0.8 + 0.2 * i / len(unique_frames), desc=f"å¤„ç†ç¬¬ {i+1}/{len(unique_frames)} å¸§...")
                
                markdown_content += f"\n### å¸§ {i+1}\n\n"
                ppt_content = self.processor.extract_ppt_content(frame, frame_num, fps)
                markdown_content += ppt_content
                markdown_content += "\n---\n"
            
            # ä¿å­˜markdownæ–‡ä»¶
            md_path = tempfile.mktemp(suffix=".md")
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
            os.remove(audio_path)
            for chunk_path, _ in audio_chunks:
                if os.path.exists(chunk_path):
                    os.remove(chunk_path)
            
            progress(1.0, desc="å¤„ç†å®Œæˆï¼")
            
            # é‡æ–°åˆ›å»ºéŸ³é¢‘æ–‡ä»¶ä¾›ä¸‹è½½
            final_audio_path = self.processor.extract_audio(video)
            
            return final_audio_path, transcript_json, md_path, "å¤„ç†å®Œæˆï¼æ‰€æœ‰æ–‡ä»¶å·²å‡†å¤‡å¥½ä¸‹è½½ã€‚"
            
        except Exception as e:
            logger.error(f"Processing error: {e}")
            return None, None, None, f"å¤„ç†å‡ºé”™: {str(e)}"

# åˆ›å»ºGradioåº”ç”¨
def create_app():
    interface = GradioInterface()
    
    with gr.Blocks(title="è§†é¢‘PPTæå–ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¥ è§†é¢‘PPTå†…å®¹æå–ç³»ç»Ÿ")
        gr.Markdown("""
        ### åŠŸèƒ½è¯´æ˜ï¼š
        1. **éŸ³é¢‘æå–**ï¼šä»è§†é¢‘ä¸­æå–éŸ³é¢‘ï¼Œæ”¯æŒ1å°æ—¶åˆ†å—å¤„ç†
        2. **è¯­éŸ³è½¬å½•**ï¼šä½¿ç”¨Whisperæ¨¡å‹è½¬å½•éŸ³é¢‘å†…å®¹
        3. **å¸§æå–**ï¼šæŒ‰æŒ‡å®šé—´éš”æå–è§†é¢‘å¸§ï¼Œæ”¯æŒROIé€‰æ‹©
        4. **æ™ºèƒ½å»é‡**ï¼šä½¿ç”¨CLIPæ¨¡å‹å»é™¤ç›¸ä¼¼å¸§
        5. **å†…å®¹æå–**ï¼šåˆ†æPPTå†…å®¹å¹¶ç”ŸæˆMarkdownæ–‡æ¡£
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                video_input = gr.Video(label="ä¸Šä¼ è§†é¢‘", height=300)
                
                with gr.Row():
                    interval_input = gr.Number(
                        label="å¸§æå–é—´éš”ï¼ˆç§’ï¼‰",
                        value=5,
                        minimum=1,
                        maximum=60,
                        step=1
                    )
                    threshold_input = gr.Slider(
                        label="ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆè¶Šé«˜è¶Šä¸¥æ ¼ï¼‰",
                        minimum=0.8,
                        maximum=1.0,
                        value=0.95,
                        step=0.01
                    )
                
                with gr.Row():
                    extract_audio_btn = gr.Button("ä»…æå–éŸ³é¢‘", variant="secondary")
                    process_btn = gr.Button("å®Œæ•´å¤„ç†", variant="primary")
                
                status_text = gr.Textbox(label="çŠ¶æ€ä¿¡æ¯", lines=3, interactive=False)
                
            with gr.Column(scale=1):
                preview_image = gr.Image(
                    label="é¢„è§ˆå¸§ï¼ˆç‚¹å‡»ä¸¤æ¬¡é€‰æ‹©ROIåŒºåŸŸï¼‰",
                    interactive=True,
                    height=400
                )
        
        with gr.Row():
            with gr.Column():
                audio_output = gr.File(label="ğŸ“ æå–çš„éŸ³é¢‘", interactive=False)
            with gr.Column():
                transcript_output = gr.File(label="ğŸ“ è½¬å½•ç»“æœ (JSON)", interactive=False)
            with gr.Column():
                markdown_output = gr.File(label="ğŸ“ PPTå†…å®¹ (Markdown)", interactive=False)
        
        # ä½¿ç”¨ç¤ºä¾‹
        gr.Markdown("""
        ### ä½¿ç”¨æ­¥éª¤ï¼š
        1. ä¸Šä¼ è§†é¢‘æ–‡ä»¶
        2. åœ¨é¢„è§ˆå¸§ä¸Šç‚¹å‡»ä¸¤æ¬¡é€‰æ‹©æ„Ÿå…´è¶£çš„åŒºåŸŸï¼ˆå¯é€‰ï¼‰
        3. è®¾ç½®å¸§æå–é—´éš”å’Œç›¸ä¼¼åº¦é˜ˆå€¼
        4. ç‚¹å‡»"å®Œæ•´å¤„ç†"å¼€å§‹å¤„ç†
        5. ç­‰å¾…å¤„ç†å®Œæˆåä¸‹è½½ç»“æœæ–‡ä»¶
        
        ### æ³¨æ„äº‹é¡¹ï¼š
        - é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…
        - å¤„ç†å¤§æ–‡ä»¶å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
        - å»ºè®®ä½¿ç”¨GPUåŠ é€Ÿå¤„ç†
        """)
        
        # äº‹ä»¶å¤„ç†
        video_input.change(
            interface.upload_video,
            inputs=[video_input],
            outputs=[preview_image, status_text, audio_output]
        )
        
        preview_image.select(
            interface.handle_click,
            inputs=[],
            outputs=[preview_image, status_text, audio_output]
        )
        
        extract_audio_btn.click(
            interface.extract_audio_only,
            inputs=[video_input],
            outputs=[audio_output, status_text]
        )
        
        process_btn.click(
            interface.process_video_full,
            inputs=[video_input, interval_input, threshold_input],
            outputs=[audio_output, transcript_output, markdown_output, status_text]
        )
    
    return demo

# è¿è¡Œåº”ç”¨
if __name__ == "__main__":
    app = create_app()

    # ä¿®æ”¹ä¸ºï¼š
    app.launch(
    share=True,
    max_threads=10
    )